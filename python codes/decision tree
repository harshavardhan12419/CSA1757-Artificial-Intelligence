import numpy as np

class DecisionTree:
    def __init__(self, min_samples_split=2):
        self.min_samples_split = min_samples_split
        self.tree = None

    def entropy(self, y):
        """Calculate the entropy of a dataset."""
        labels, counts = np.unique(y, return_counts=True)
        probabilities = counts / counts.sum()
        return -np.sum(probabilities * np.log2(probabilities + 1e-9))

    def information_gain(self, X_column, y):
        """Calculate the information gain of a feature."""
        total_entropy = self.entropy(y)
        values, counts = np.unique(X_column, return_counts=True)
        weighted_entropy = 0

        for i in range(len(values)):
            subset_y = y[X_column == values[i]]
            weighted_entropy += (counts[i] / np.sum(counts)) * self.entropy(subset_y)

        return total_entropy - weighted_entropy

    def best_split(self, X, y):
        """Find the best feature and value to split on."""
        best_gain = -1
        best_feature = None

        for feature_index in range(X.shape[1]):
            X_column = X[:, feature_index]
            gain = self.information_gain(X_column, y)

            if gain > best_gain:
                best_gain = gain
                best_feature = feature_index

        return best_feature

    def build_tree(self, X, y):
        """Recursively build the decision tree."""
        if len(np.unique(y)) == 1:
            return y[0]
        
        if len(y) < self.min_samples_split:
            return np.bincount(y).argmax()

        best_feature = self.best_split(X, y)
        if best_feature is None:
            return np.bincount(y).argmax()

        tree = {best_feature: {}}
        values = np.unique(X[:, best_feature])

        for value in values:
            subset_X = X[X[:, best_feature] == value]
            subset_y = y[X[:, best_feature] == value]
            tree[best_feature][value] = self.build_tree(subset_X, subset_y)

        return tree

    def fit(self, X, y):
        """Fit the decision tree model."""
        self.tree = self.build_tree(X, y)

    def predict_sample(self, sample, tree):
        """Predict a single sample using the decision tree."""
        if not isinstance(tree, dict):
            return tree

        feature_index = list(tree.keys())[0]
        feature_value = sample[feature_index]
        subtree = tree[feature_index].get(feature_value)

        if subtree is None:
            return None  # or some default value

        return self.predict_sample(sample, subtree)

    def predict(self, X):
        """Predict labels for a set of samples."""
        return [self.predict_sample(sample, self.tree) for sample in X]

# Example usage
if __name__ == "__main__":
    # Sample dataset: [feature1, feature2, label]
    X = np.array([[1, 2], [1, 3], [2, 2], [2, 3], [3, 3], [3, 4]])
    y = np.array([0, 0, 1, 1, 1, 1])  # Labels

    # Create and train the decision tree
    tree = DecisionTree(min_samples_split=2)
    tree.fit(X, y)

    # Predict on new samples
    predictions = tree.predict(X)
    print("Predictions:", predictions)

    # Sample output
    # Predictions: [0, 0, 1, 1, 1, 1]
